---
title: "Resistant lines"
output:
  html_document:
    toc: yes
    toc_depth: 3
  word_document: default
---

```{r echo = FALSE, message = FALSE}
knitr::knit_hooks$set(small.mar = function(before, options, envir) {
    if (before) par(mar = c(4, 4, 1, 1))  # smaller margin on top and right
})

knitr::opts_chunk$set(
  comment = "",
  message = FALSE,
  tidy = FALSE,
  dev=c('CairoPNG', 'CairoPDF'),
  cache = FALSE,
  warning = FALSE,
  encoding = "UTF-8")
```

# Introduction

Ordinary least squares regression lines (those created with the `lm()` function) suffer from sensitivity to outliers. Because `lm`'s best fit line makes use of the mean (which is not a robust measure of location), its breakdown point is $1/n$  meaning that all it takes is for one data point to behave differently from the rest of the points to significantly alter the slope and intercept of the best fit line. For example, let's start with a well behaved dataset where all points are perfectly aligned and fit this batch with a regression line:

```{r fig.height=2.5, fig.width=3, small.mar=TRUE}
x <- seq(1:11)
y <- 5 + 2 * x
plot(y ~ x, pch=20)
M <- lm(y ~ x)
abline( M , col="red")
```

As expected, we have a perfect fit. And the regression model's coefficients match those used to create the data.

```{r}
coef(M)
```


Now, what if one of the points is re-positioned in the plot, what happens to the regression line?

```{r fig.height=2.5, fig.width=3, small.mar=TRUE}
y[11] <- 2
plot(y ~ x, pch=20)
M <- lm(y ~ x)
abline( M , col="red")
```

Note the significant change in the line's characteristics, it's intercept and slope are now:

```{r}
coef(M)
```

The slope dropped from 2 to 0.86 because of a single point!

If our goal is to explore what the bulk of the data has to say about the phenomena being investigated, we certainly don't want a small batch of "maverick" values to hijack the analysis. We therefore need a set of fitting tools that minimize the influence of outliers. There are many options out there; most notable are Tukey's **3-point summary line** and the **bisquare** robust estimation method outlined in Cleveland's text.

# Robust lines

## Tukey's 3-point summary

The idea is simple in concept and easy to implement with a pen and paper is the dataset is not too big. It involves dividing the dataset into three approximately equal groups and summarizing these groups by computing their respective medians. Two half-slopes are then used to join the three points. Points that share the same x value are lumped into the same batch which can lead to unequal group sizes. The motivation behind this plot is to use the three-point summary to provide a robust assessment of the type of relationship between both variables. Such plots are often used to help guide re-expression of the variables x or y or both for the sole purpose of straightening the x-y relationship.

Let's look at an example using the last (modified) dataset.  First,we'll divide the plot into three approximately equal batches.


```{r echo=FALSE, fig.show='hide'}
source("http://www.colby.edu/~mgimond/R/MG_EDA.R")
r.lm <- sum3pt(x=x, y=y, dir=F)

```

```{r fig.height=2.5, fig.width=3,  small.mar=TRUE,echo=FALSE}
# Find the three parts
x.u <- unique(x)
n <- length( unique(match(x, x.u[order(x.u)] )) )
span <- floor(n/3)
r    <- n %% 3
# Compute the max index of each third
if( r == 0) d <- c(span, span *2, span *3)
if( r == 1) d <- c(span, span *2 + 1, span *3 + 1)
if( r == 2) d <- c(span + 1, span * 2 + 1, span *3 + 2)
  
plot(x,y, pch=20)
abline(v= d[1:2],lty=2,col="grey80")
```

Next, compute the median x and y values within each section. 

```{r fig.height=2.5, fig.width=3,  small.mar=TRUE,echo=FALSE}
plot(x,y, pch=20)
abline(v= d[1:2],lty=2,col="grey80")
points(cbind(r.lm$xmed, r.lm$ymed), pch=16,col="red")

```

Note that here, we do not include a same point in two median calculations. This implies that for the mid-third of the data, we do not include the point straddling the left boundary line when computing its median value. Likewise with the right-third of the data. The x median values are `r r.lm$xmed` and the y median values are `r r.lm$ymed`.

Finally, we fit the tail end medians with a straight lines.

```{r fig.height=2.5, fig.width=3,  small.mar=TRUE,echo=FALSE}
plot(x,y, pch=20)
abline(v= d[1:2],lty=2,col="grey80")
points(cbind(r.lm$xmed, r.lm$ymed), pch=16,col="red")
lines(cbind(r.lm$xmed[-2], r.lm$ymed[-2]),lty=1,col="#444444")
```

This gives us the slope of the line (we can, of course, extend the lines to the boundaries of the x range of values). Note that we have not completely eliminated the influence of the outlier; this is not our goal. The outlier does take part in helping fit the line, but it doesn't wield disproportionate influence on the line. 

So what purpose does the middle median serve? It helps us adjust the vertical location of the line. The goal is to nudge the line upward or downward about 1/3 of the way towards the middle median value (parallel to the y-axis). This then defines the line's *intercept* (which is where the line crosses the y-axis when x = 0).

```{r fig.height=2.5, fig.width=3,  small.mar=TRUE,echo=FALSE}
plot(x,y, pch=20)
abline(v= d[1:2],lty=2,col="grey80")
points(cbind(r.lm$xmed, r.lm$ymed), pch=16,col="red")
lines(cbind(r.lm$xmed[-2], r.lm$ymed[-2] + 0.4),lty=1,col="#444444")
```


## Bisquare

In his book (pages 112 - 119), Cleveland uses the *bisquare* estimation method to come up with a robust line.

```{r}
wt.bisquare <- function(u, c=6) {
   ifelse( abs(u/c) < 1, (1-(u/c)^2)^2, 0)
}

wt <- rep(1,length(x))

plot(x,y, pch=20)
for(i in 1:10){
  dat.lm <- lm(y ~ x ,weights=wt)
  wt <- wt.bisquare( dat.lm$res/ median(abs(dat.lm$res)), c=6 )
}
abline(dat.lm, col=rgb(1,0,0,0.3))

```

## Sample data

```{r}
library(lubridate)
library(dplyr)
df <- read.csv("http://mgimond.github.io/ES218/Data/GoM_hist.csv")

df2 <- df %>%
       mutate(Date = mdy(Date),
              Year = year(Date)) %>%
       filter(Year > 1950) %>%
       group_by( Year) %>%
       summarize( AvgTemp = mean(Temp, na.rm=T)) 

plot(AvgTemp ~ Year,df2, pch=16, col=rgb(0,0,0,0.2))
abline( lm( AvgTemp ~ Year, dat=df2) , col="red")
```


```{r}

wt <- rep(1,length(df2$Year))

plot(AvgTemp ~ Year,df2, pch=16, col=rgb(0,0,0,0.2))
for(i in 1:10){
  dat.lm <- lm(AvgTemp ~ Year,df2 ,weights=wt)
  wt <- wt.bisquare( dat.lm$res/ median(abs(dat.lm$res)), c=6 )
}

# Plot the robust line
abline( dat.lm, col="red")

# Plot the default regression line
abline( lm( AvgTemp ~ Year, dat=df2) , col="grey", lty=2)
```


# Robust loess

The bisquare 

Default loess

```{r}
# Fit a robust loess

lo <- loess(AvgTemp ~ Year,df2, span=1/3)

plot(AvgTemp ~ Year,df2, pch=16, col=rgb(0,0,0,0.2))
lines(df2$Year, predict(lo), col="red")
```



```{r}
# Fit a robust loess
wt <- rep(1,length(df2$Year))

plot(AvgTemp ~ Year,df2, pch=16, col=rgb(0,0,0,0.2))
for(i in 1:10){
  dat.lm <- loess(AvgTemp ~ Year,df2, weights = wt, span=1/3)
  wt <- wt.bisquare( dat.lm$res/ median(abs(dat.lm$res)), c=6 )
}

# Robust loess
lines(df2$Year, predict(dat.lm), col="red")

# Add default loess
lines(df2$Year, predict(lo), col="grey", lty=2)

```





